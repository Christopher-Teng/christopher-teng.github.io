<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"christopher-teng.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="在上一篇文章中，我们已经完成了对目标网站的分析(中华诗词网)，接下来开始进行爬虫开发。 首先启动一个新项目，搭建基本目录结构，这里推荐使用 virtualenv 创建 python 虚拟环境进行开发。 12345mkdir poems_crawler &amp;&amp; cd $_virtualenv -p Python3 venv. venv&#x2F;bin&#x2F;activate 以上命令创建了一个 po">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Scrapy爬取诗词数据（续）">
<meta property="og:url" content="https://christopher-teng.github.io/2021/05/03/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%AD%EF%BC%89/index.html">
<meta property="og:site_name" content="Christopher Teng&#39;s Blog">
<meta property="og:description" content="在上一篇文章中，我们已经完成了对目标网站的分析(中华诗词网)，接下来开始进行爬虫开发。 首先启动一个新项目，搭建基本目录结构，这里推荐使用 virtualenv 创建 python 虚拟环境进行开发。 12345mkdir poems_crawler &amp;&amp; cd $_virtualenv -p Python3 venv. venv&#x2F;bin&#x2F;activate 以上命令创建了一个 po">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-05-03T11:08:49.000Z">
<meta property="article:modified_time" content="2021-08-22T11:53:40.110Z">
<meta property="article:author" content="滕飞">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Scrapy">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://christopher-teng.github.io/2021/05/03/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%AD%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>使用Scrapy爬取诗词数据（续） | Christopher Teng's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Christopher Teng's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Christopher Teng's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">关注技术、持续提升</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">12</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Christopher-Teng" class="github-corner" title="Christopher-Teng Github" aria-label="Christopher-Teng Github" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://christopher-teng.github.io/2021/05/03/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%AD%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="滕飞">
      <meta itemprop="description" content="学海无涯，争渡争渡......">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Christopher Teng's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用Scrapy爬取诗词数据（续）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-03 19:08:49" itemprop="dateCreated datePublished" datetime="2021-05-03T19:08:49+08:00">2021-05-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-22 19:53:40" itemprop="dateModified" datetime="2021-08-22T19:53:40+08:00">2021-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在上一篇文章中，我们已经完成了对目标网站的分析(<a target="_blank" rel="noopener" href="https://www.zhsc.net">中华诗词网</a>)，接下来开始进行爬虫开发。</p>
<p>首先启动一个新项目，搭建基本目录结构，这里推荐使用 virtualenv 创建 python 虚拟环境进行开发。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir poems_crawler &amp;&amp; cd $_</span><br><span class="line"></span><br><span class="line">virtualenv -p Python3 venv</span><br><span class="line"></span><br><span class="line">. venv/bin/activate</span><br></pre></td></tr></table></figure>
<p>以上命令创建了一个 poems_crawler 空目录，并进入目录启激活 python3 虚拟环境。</p>
<p>接下来安装 scrapy 并创建一个 zhsc_crawler 的项目：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy3</span><br><span class="line"></span><br><span class="line">scrapy startproject zhsc_crawler</span><br></pre></td></tr></table></figure>
<p>到这里，我们使用 scrapy 新建了一个 zhsc_crawler 项目，在当前目录下多出了一个名为 zhsc_crawler 的项目根目录，进入项目目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd zhsc_crawler</span><br></pre></td></tr></table></figure>
<p>查看目录结构可以看到在项目根目录下还有一个同名的目录<code>zhsc_crawler</code>，以及一个名为 scrapy.cfg 的 scrapy 配置文件。这里的<code>zhsc_crawler</code>目录就是爬虫项目的包目录，这里也是之后我们自己编写代码的地方，其下包含了 items.py(Item 定义文件)、middlewares.py(自定义中间件)、pipelines.py(自定义管道)、settings.py(项目配置文件)和一个名为<code>spiders</code>的目录，这是放置爬虫类的包目录。</p>
<span id="more"></span>
<p>现在来创建我们的蜘蛛类：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider poems www.zhsc.net</span><br></pre></td></tr></table></figure>
<p>现在在<code>spiders</code>目录下会生成一个 poems.py 文件，我们在这里来编写蜘蛛。</p>
<p>首先修改其中的<code>start_urls</code>，也就是爬虫开始发起请求的地址，根据前面的分析，种子页应该是作者列表页：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_urls=[<span class="string">&#x27;https://www.zhsc.net/Index/shi_more.html&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>现在来编写处理种子页响应的代码，在<code>poems.py</code>中，对<code>start_urls</code>发起的请求，其响应会被<code>parse</code>方法处理，因此先来看看在<code>parse</code>方法里面怎么写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">author=<span class="built_in">getattr</span>(self,<span class="string">&#x27;author&#x27;</span>,<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>前面说过，爬虫将被设计为爬取指定作者的诗词数据，在 scrapy 中提供了通过 scrapy cli 向蜘蛛中动态传入参数的功能，其方式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl poems -a author=李白</span><br></pre></td></tr></table></figure>
<p>通过<code>-a</code>选项传入的参数会被传递给蜘蛛类的<code>__init__</code>方法成为蜘蛛实例上的一个属性，因此通过 getattr 方法便可以获取由命令行传入的参数。</p>
<p>下一步，对作者列表进行筛选，匹配指定的作者。</p>
<p>首先因该从页面中提取所有作者的链接，在 scrapy 中可以使用 xpath 和 css 两种方式来获取 html 内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">author_links=response.css(<span class="string">&#x27;.ci_lei1&gt;.ci_lei1&gt;.ci_lei1_xuan&gt;.ci_lei1_xuan2 a&#x27;</span>).getall()</span><br></pre></td></tr></table></figure>
<p><code>response.css</code>是使用 scrapy 的 css 选择器获取 html 内容，得到的结果可以通过<code>get</code>和<code>getall</code>方法来取得第一项匹配结果或所有匹配结果的列表。这里使用<code>getall</code>得到当前页面包含的所有作者链接。</p>
<p>接下来从所有作者中匹配指定的作者：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> author <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">  <span class="keyword">for</span> link <span class="keyword">in</span> author_links:</span><br><span class="line">    poems_list_page_url=find_author_url(link,author)</span><br><span class="line">    <span class="keyword">if</span> poems_list_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">yield</span> response.follow(poems_list_page_url,self.parse_list)</span><br><span class="line"><span class="keyword">return</span> ZhscCrawlerItem()</span><br></pre></td></tr></table></figure>
<p>这里我们使用<code>for...in</code>遍历作者列表来进行匹配，这里引入的自定义的方法<code>find_author_url</code>，如果调用该方法成功返回了链接地址，则使用 scrapy 提供的<code>response.follow</code>方法让蜘蛛沿着指定的链接继续爬取，最后<code>return ZhscCrawlerItem()</code>是返回爬取到的数据，这是新建爬虫项目时，scrapy 自动为我们在<code>items.py</code>中生成的数据类，稍后我们会在这里对爬取数据进行定义。</p>
<p>先来看看自定义的<code>find_author_url</code>方法，其定义在项目目录下的<code>processors.py</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_author_url</span>(<span class="params">link,author</span>):</span></span><br><span class="line">  <span class="keyword">if</span> re.search(<span class="string">r&#x27;&gt;\s*&#123;&#125;\s*&lt;&#x27;</span>.<span class="built_in">format</span>(author),link) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">return</span> re.search(<span class="string">r&#x27;(?:href\s*\=\s*&quot;)(.+?)(?:&quot;)&#x27;</span>,link).group(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>逻辑很简单，使用正则先匹配指定作者，如果找到则返回其链接，否则返回<code>None</code>。</p>
<p>现在如果我们传入作者名字并且假定这是一个被收录在目标网站中的作者，那么蜘蛛将会进入诗词列表页面，下面来看看怎么编写处理诗词列表页面的方法<code>parse_list</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">detail_urls=response.css(<span class="string">&#x27;.zh_sou_jie&gt;.zh_jie_con a::attr(href)&#x27;</span>).getall()</span><br></pre></td></tr></table></figure>
<p>scrapy 的 css 选择器使用<code>::attr</code>的语法来提取 html 标签中的属性，这里首先提取当前诗词列表页面中的所有诗词详情页地址。然后让蜘蛛跟随详情页链接，进入下一步详情页爬取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> detail_urls:</span><br><span class="line">  <span class="keyword">yield</span> response.follow(url,self.parse_item)</span><br></pre></td></tr></table></figure>
<p>之前分析过，诗词详情由于条目众多，目标网站使用了分页器，因此在诗词列表页的处理中还因该让蜘蛛跟随分页器的链接继续爬取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">next_page_urls=response.css(<span class="string">&#x27;.page a::attr(href)&#x27;</span>).getall()</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> next_page_urls:</span><br><span class="line">  <span class="keyword">yield</span> response.follow(url,self.parse_list)</span><br></pre></td></tr></table></figure>
<p>到这一步，蜘蛛应该可以顺利到达所有的诗词详情页，接下来编写蜘蛛的核心代码，对数据页的爬取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self,response</span>):</span></span><br><span class="line">  loader=ItemLoader(item=ZhscCrawlerItem(),response=response)</span><br></pre></td></tr></table></figure>
<p>这里使用了两个新类<code>ItemLoader</code>和<code>ZhscCrawlerItem</code>，上面提到过，<code>ZhscCrawlerItem</code>是新建爬虫时 scrapy 自动为我们创建的数据类，而<code>ItemLoader</code>则是 scrapy 为我们提供的数据处理类，可以直接引入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br></pre></td></tr></table></figure>
<p>首先来定义数据类，根据前文分析，要爬取的数据主要包含标题 title、年代 times、作者 author 和正文 content，在项目目录下的<code>items.py</code>中定义数据结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itemloaders.processors <span class="keyword">import</span> Join,MapCompose,TakeFirst</span><br><span class="line"><span class="keyword">from</span> scrapy.item <span class="keyword">import</span> Field,Item</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .processors <span class="keyword">import</span> get_author,get_times,parse_content</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhscCrawlerItem</span>(<span class="params">Item</span>):</span></span><br><span class="line">  title=Field(input_processor=MapCompose(<span class="built_in">str</span>.strip,stop_on_none=<span class="literal">True</span>),output_processor=TakeFirst())</span><br><span class="line">  times=Field(input_processor=MapCompose(get_times,stop_on_none=<span class="literal">True</span>),output_processor=TakeFirst())</span><br><span class="line">  author=Field(input_processor=MapCompose(get_author,stop_on_none=<span class="literal">True</span>),output_processor=TakeFirst())</span><br><span class="line">  content=Field(input_processor=MapCompose(parse_content,stop_on_none=<span class="literal">True</span>),output_processor=Join(<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>scrapy 中定义数据结构使用<code>Item</code>和<code>Field</code>基类，使用语法<code>字段名=Field()</code>来规定数据因该包含的字段。</p>
<p>前文中已经分析过，对详情页中原始的标题、年代、作者和正文数据因该先进行处理，让我们回顾一下：</p>
<ol>
<li>标题要修改为原始标题加上正文内容的第一句</li>
<li>年代和作者信息位于同一个<code>&lt;p&gt;</code>标签内，要分别进行提取</li>
<li>正文结构不尽相同，有些诗词正文中还带有注释内容，需要进行过滤</li>
</ol>
<p>在scrapy中当我们要爬取指定页面时，首先会由spider发起一个爬虫请求给scrapy engine，然后engine将请求添加到scheduler的调度队列，当接到scheduler的响应后engine通知downloader对指定url开始下载数据，完成下载之后，将数据交给spider解析。</p>
<p>所以对于上面需要进行的数据处理，就可以放在spider对downloader下载回来的数据进行解析的时候。</p>
<p>在<code>Field</code>中，使用<code>input_processor</code>和<code>output_processor</code>就可以分别在downloader下载完数据时进行处理，以及在spider解析完数据向后传递给pipeline之前进行处理。而<code>itemloaders.processors</code>中提供了<code>Join</code>、<code>MapCompose</code>和<code>TakeFirst</code>三个数据处理方法，其中<code>Join</code>和<code>TakeFirst</code>用于<code>output_processor</code>，顾名思义，这两个方法分别用于将数据列表中的每一项合并后输出，和获取数据列表中的第一项输出。</p>
<p><code>MapCompose</code>方法用于<code>input_processor</code>，可以传入多个方法，<code>MapCompose</code>会将获取的数据依次传递给这些方法处理，然后将结果交给向后传递。由于获取的原始数据可能是一个列表，比如上面的诗词正文，在详情页中，正文是位于一个<code>&lt;div&gt;</code>标签下的多个文本节点组成的，因此通过 scrapy 的 css 选择器得到的是一个列表，假如列表中某一项为空值，而我们指定的处理方法可能无法处理输入值为空的情况，这是便会报错，所以可以在<code>MapCompose</code>中指定<code>stop_on_none=True</code>来规定遇到空值时停止继续处理。</p>
<p><code>get_author</code>、<code>get_times</code>和<code>parse_content</code>是<code>processors.py</code>中的三个自定义的处理函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_author</span>(<span class="params">value</span>):</span></span><br><span class="line">  result=re.search(<span class="string">r&#x27;(?:作者:\s*)(.*)(?:\s*)&#x27;</span>,value)</span><br><span class="line">  <span class="keyword">return</span> result.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_times</span>(<span class="params">value</span>):</span></span><br><span class="line">  result=re.search(<span class="string">r&#x27;(?:年代:\s*)(.+?)(?:\s)&#x27;</span>,value)</span><br><span class="line">  <span class="keyword">return</span> result.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_content</span>(<span class="params">value</span>):</span></span><br><span class="line">  value=remove_tags(value.strip())</span><br><span class="line">  <span class="keyword">return</span> re.sub(<span class="string">r&#x27;(。|！|？)&#x27;</span>,<span class="string">r&#x27;\1\n&#x27;</span>,value)</span><br></pre></td></tr></table></figure>
<p>这里使用了 python 标准库<code>w3lib</code>里面的 remove_tags 方法去除 html 标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> w3lib.html <span class="keyword">import</span> remove_tags</span><br></pre></td></tr></table></figure>
<p><code>re.sub(r&#39;(。|！|？)&#39;,r&#39;\1\n&#39;,value)</code>在句号或感叹号或问号后面添加一个换行，这是为了符合阅读诗词时的习惯：在一个完整的诗句后面换行。</p>
<p>在这里并没有对标题进行处理(将标题修改为原始标题加上正文第一句)，因为在这里主要是定义数据结构，使用<code>input_processor</code>和<code>output_processor</code>的目的是正确提取数据，规范数据内容，而对标题的处理属于项目设计的需求，因此将放到后面自定义管道中再进行。</p>
<p>回到蜘蛛<code>poems.py</code>中，现在处理详情页的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self,response</span>):</span></span><br><span class="line">  loader=ItemLoader(item=ZhscCrawlerItem(),response=response)</span><br></pre></td></tr></table></figure>
<p>接着往下，向<code>loader</code>中传入定义的字段数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loader.add_css(<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;span::text,.zh_shi_xiang1&gt;span&gt;*::text&#x27;</span>)</span><br><span class="line">loader.add_css(<span class="string">&#x27;times&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;p::text&#x27;</span>)</span><br><span class="line">loader.add_css(<span class="string">&#x27;author&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;p::text&#x27;</span>)</span><br><span class="line">loader.add_css(<span class="string">&#x27;content&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;div::text,.zh_shi_xiang1&gt;div&gt;*::text&#x27;</span>)</span><br><span class="line"><span class="keyword">return</span> loader.load_item()</span><br></pre></td></tr></table></figure>
<p><code>loader.add_css</code>方法使用 scrapy 的 css 选择器将 html 中的内容添加到在<code>ZhscCrawlerItem</code>中定义的指定字段，最后使用<code>loader.load_item</code>方法将获取的数据返回，这里要注意使用<code>add_css</code>方法后，只是 item 的指定字段注入数据，但是数据并不会被返回，所以在完成向所有字段注入数据后，一定要调用<code>load_item</code>方法将数据返回。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Scrapy/" rel="tag"># Scrapy</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/" rel="prev" title="使用Scrapy爬取诗词数据">
      <i class="fa fa-chevron-left"></i> 使用Scrapy爬取诗词数据
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/05/04/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%88%EF%BC%89/" rel="next" title="使用Scrapy爬取诗词数据（终）">
      使用Scrapy爬取诗词数据（终） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">滕飞</p>
  <div class="site-description" itemprop="description">学海无涯，争渡争渡......</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Christopher-Teng</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  

  

    </div>
</body>
</html>
