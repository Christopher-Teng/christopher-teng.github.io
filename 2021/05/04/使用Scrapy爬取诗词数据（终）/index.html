<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"christopher-teng.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="前文中，我们已经成功从诗词详情页爬取到了数据，接下来对数据内容根据需求进行加工处理以及存储。 首先，对标题进行处理，将其改为原标题加上正文第一句的形式。对数据的处理可以通过自定义管道来实现，scrapy 的管道可以将数据一层一层的进行复杂逻辑的处理，每一个管道类都需要返回 item，已保证数据可以在多个管道类之间流动。 使用 scrapy 创建爬虫项目时，会在项目目录下自动创建一个pipeline">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Scrapy爬取诗词数据（终）">
<meta property="og:url" content="https://christopher-teng.github.io/2021/05/04/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%88%EF%BC%89/index.html">
<meta property="og:site_name" content="Christopher Teng&#39;s Blog">
<meta property="og:description" content="前文中，我们已经成功从诗词详情页爬取到了数据，接下来对数据内容根据需求进行加工处理以及存储。 首先，对标题进行处理，将其改为原标题加上正文第一句的形式。对数据的处理可以通过自定义管道来实现，scrapy 的管道可以将数据一层一层的进行复杂逻辑的处理，每一个管道类都需要返回 item，已保证数据可以在多个管道类之间流动。 使用 scrapy 创建爬虫项目时，会在项目目录下自动创建一个pipeline">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-05-04T08:27:07.000Z">
<meta property="article:modified_time" content="2021-08-22T11:53:40.110Z">
<meta property="article:author" content="滕飞">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Scrapy">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://christopher-teng.github.io/2021/05/04/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%88%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>使用Scrapy爬取诗词数据（终） | Christopher Teng's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Christopher Teng's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Christopher Teng's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">关注技术、持续提升</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">12</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Christopher-Teng" class="github-corner" title="Christopher-Teng Github" aria-label="Christopher-Teng Github" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://christopher-teng.github.io/2021/05/04/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%88%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="滕飞">
      <meta itemprop="description" content="学海无涯，争渡争渡......">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Christopher Teng's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用Scrapy爬取诗词数据（终）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-04 16:27:07" itemprop="dateCreated datePublished" datetime="2021-05-04T16:27:07+08:00">2021-05-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-22 19:53:40" itemprop="dateModified" datetime="2021-08-22T19:53:40+08:00">2021-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>前文中，我们已经成功从诗词详情页爬取到了数据，接下来对数据内容根据需求进行加工处理以及存储。</p>
<p>首先，对标题进行处理，将其改为原标题加上正文第一句的形式。对数据的处理可以通过自定义管道来实现，scrapy 的管道可以将数据一层一层的进行复杂逻辑的处理，每一个管道类都需要返回 item，已保证数据可以在多个管道类之间流动。</p>
<p>使用 scrapy 创建爬虫项目时，会在项目目录下自动创建一个<code>pipelines.py</code>文件，其中已经预定义了一个和项目同名的管道类，我们就从这里开始编写数据处理逻辑：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">from</span> scrapy.exception <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .processors <span class="keyword">import</span> modify_title</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhscCrawlerPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.logger=logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">        adapter = ItemAdapter(item)</span><br><span class="line">        <span class="keyword">if</span> adapter[<span class="string">&#x27;title&#x27;</span>] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> adapter[<span class="string">&#x27;content&#x27;</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem()</span><br><span class="line">        adapter[<span class="string">&#x27;title&#x27;</span>] = modify_title(adapter[<span class="string">&#x27;title&#x27;</span>], adapter[<span class="string">&#x27;content&#x27;</span>])</span><br><span class="line">        self.logger.debug(<span class="string">u&#x27;标题已成功经过修改，格式为：原标题 —— 诗文内容第一行 -- %(title)s&#x27;</span>, &#123;<span class="string">&#x27;title&#x27;</span>: adapter[<span class="string">&#x27;title&#x27;</span>]&#125;)</span><br><span class="line">        spider.crawler.stats.inc_value(<span class="string">&#x27;title_modify/modified&#x27;</span>, spider=spider)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>这里引入了<code>ItemAdapter</code>，这是 scrapy 提供的用于处理 item 的类，可以通过字段名获取 item 中的数据。</p>
<p>首先提取<code>title</code>字段和<code>content</code>字段的值，并且如果标题为空或者正文为空时，抛出一个<code>DropItem</code>，这是 scrapy 提供的用于丢弃数据的类，此处诗词数据如果没有标题或正文，则将该条数据抛弃。</p>
<p>然后调用在<code>processors.py</code>中自定义的标题处理方法<code>modify_title</code>，将处理后的返回值赋值给<code>title</code>字段以覆盖原标题，最后使用 python 的标准库<code>logging</code>打印日志，方便调试。</p>
<p><code>spider.crawler.stats.inc_value</code>是 scrapy 内置的状态记录方法，同样用于调试时查看爬虫运行情况。</p>
<p>下面是<code>modify_title</code>的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modify_title</span>(<span class="params">title, content</span>):</span></span><br><span class="line">    first_line = re.match(<span class="string">r&#x27;(.+)(?=\n)&#x27;</span>, content).group()</span><br><span class="line">    title = title+<span class="string">&#x27; —— &#x27;</span>+first_line</span><br><span class="line">    <span class="keyword">return</span> title</span><br></pre></td></tr></table></figure>
<p>处理逻辑很简单，使用正则提取正文第一句，由于在 item 中定义<code>content</code>字段时，使用了<code>parse_content</code>方法把正文的每一个完整诗句进行换行，所以我们只需要匹配第一个换行符前面的内容就可以得到正文第一句。处理后返回的结果为：“原标题 —— 正文第一句”。</p>
<p>到此，可以运行爬虫爬取符合需求的诗词数据了，接下来对数据进行存储。最简单的存储方式就是在<code>settings.py</code>爬虫配置文件中，配置<code>FEEDS</code>字段，scrapy 默认支持的存储方式主要有：Local filesystem、FTP、S3、Google Cloud Storage 和 Standard output，这里我们配置为使用 JSON Lines 格式将数据存储到项目根目录下的<code>poems</code>目录下，文件名为<code>poems.jsonl</code>，该格式类似 JSON，使用逐行的方式存储数据，每行都是一个标准的 JSON 字符串，其优点一是方便追加数据，二是每一行都是一个标准的 JSON 格式字符串，所以使用的时候可以按行读取数据，避免文件过大，一次全部读取造成性能瓶颈。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FEEDS=&#123;</span><br><span class="line">    pathlib.Path(<span class="string">&#x27;poems/poems.jsonl&#x27;</span>):&#123;</span><br><span class="line">        <span class="string">&#x27;format&#x27;</span>:<span class="string">&#x27;jsonlines&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;encoding&#x27;</span>:<span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>相比于直接存储与本地文件，更好的方式是使用数据库进行存储，下面在<code>pipelines.py</code>中编写一个<code>MongoDBPipeline</code>来通过自定义管道将数据存储进 MongoDB 中，在 python 中使用 mongodb 推荐使用 pymongo。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoDBPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, host=<span class="string">&#x27;127.0.0.1&#x27;</span>, port=<span class="number">27017</span>,username=<span class="string">&#x27;&#x27;</span>,password=<span class="string">&#x27;&#x27;</span>,auth_source=<span class="string">&#x27;&#x27;</span>, db_name=<span class="string">&#x27;zhsc_crawler&#x27;</span>, col_name=<span class="string">&#x27;poems&#x27;</span></span>):</span></span><br><span class="line">        self.host=host</span><br><span class="line">        self.port=port</span><br><span class="line">        self.username=username</span><br><span class="line">        self.password=password</span><br><span class="line">        self.auth_source=auth_source</span><br><span class="line">        self.db_name=db_name</span><br><span class="line">        self.col_name=col_name</span><br><span class="line">        self.logger=logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls,crawler</span>):</span></span><br><span class="line">        _host = crawler.settings.get(<span class="string">&#x27;MONGO_HOST&#x27;</span>, <span class="string">&#x27;127.0.0.1&#x27;</span>)</span><br><span class="line">        _port = crawler.settings.getint(<span class="string">&#x27;MONGO_PORT&#x27;</span>, <span class="number">27017</span>)</span><br><span class="line">        _username=crawler.settings.get(<span class="string">&#x27;MONGO_USERNAME&#x27;</span>,<span class="string">&#x27;root&#x27;</span>)</span><br><span class="line">        _password=crawler.settings.get(<span class="string">&#x27;MONGO_PASSWORD&#x27;</span>,<span class="string">&#x27;123456&#x27;</span>)</span><br><span class="line">        _auth_source=crawler.settings.get(<span class="string">&#x27;MONGO_AUTHSOURCE&#x27;</span>,<span class="string">&#x27;admin&#x27;</span>)</span><br><span class="line">        _db_name = crawler.settings.get(<span class="string">&#x27;MONGO_DB&#x27;</span>, <span class="string">&#x27;zhsc_crawler&#x27;</span>)</span><br><span class="line">        _col_name = crawler.settings.get(<span class="string">&#x27;MONGO_COLLECTION&#x27;</span>, <span class="string">&#x27;poems&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(_host, _port,_username,_password,_auth_source, _db_name, _col_name)</span><br></pre></td></tr></table></figure>
<p>上面代码首先读取 mongodb 配置，准备好进行数据库连接，其中类方法<code>from_crawler</code>是 scrapy 提供的管道类中读取爬虫配置文件的方法。</p>
<p>在<code>settings.py</code>中配置 mongodb：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MONGO_HOST = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">MONGO_PORT = <span class="number">27017</span></span><br><span class="line">MONGO_USERNAME = <span class="string">&#x27;root&#x27;</span></span><br><span class="line">MONGO_PASSWORD = <span class="string">&#x27;123456&#x27;</span></span><br><span class="line">MONGO_AUTHSOURCE = <span class="string">&#x27;admin&#x27;</span></span><br><span class="line">MONGO_DB = <span class="string">&quot;zhsc_crawler&quot;</span></span><br><span class="line">MONGO_COLLECTION = <span class="string">&quot;poems&quot;</span></span><br></pre></td></tr></table></figure>
<p>MongoDB 的相关知识这里不单独做解释，网上资料很多，而且 MongoDB 上手也很简单。这里配置使用的数据库名称为<code>zhsc_crawler</code>，集合名称<code>poems</code>。</p>
<p>对数据库的连接和关闭可以放在运行爬虫和爬虫运行完毕时，这样可以有效节省开销：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">    self.connection = pymongo.MongoClient(host=self.host, port=self.port,username=self.username,password=self.password,authSource=self.auth_source)</span><br><span class="line">    self.db = self.connection[self.db_name]</span><br><span class="line">    self.collection = self.db[self.col_name]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">    self.connection.close()</span><br></pre></td></tr></table></figure>
<p><code>open_spider</code>和<code>close_spider</code>由 scrapy 提供，用于在蜘蛛启动和关闭时自定义操作。</p>
<p>最后是管道类必须实现的方法<code>process_item</code>，用于处理数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">    adapter = ItemAdapter(item)</span><br><span class="line">    self.collection.insert_one(<span class="built_in">dict</span>(item))</span><br><span class="line">    self.logger.debug(<span class="string">u&#x27;数据已插入MongoDB！ %(title)s -- %(times)s -- %(author)s&#x27;</span>,&#123;<span class="string">&#x27;title&#x27;</span>: adapter[<span class="string">&#x27;title&#x27;</span>], <span class="string">&#x27;times&#x27;</span>: adapter[<span class="string">&#x27;times&#x27;</span>], <span class="string">&#x27;author&#x27;</span>: adapter[<span class="string">&#x27;author&#x27;</span>]&#125;)</span><br><span class="line">    spider.crawler.stats.inc_value(<span class="string">&#x27;mongodb/inserted&#x27;</span>, spider=spider)</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>最后，在<code>settings.py</code>中配置<code>ITEM_PIPELINES</code>字段，开启自定义管道，默认只开启了创建项目时 scrapy 自动创建的项目同名管道类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">    <span class="string">&#x27;zhsc_crawler.pipelines.ZhscCrawlerPipeline&#x27;</span>:<span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;zhsc_crawler.pipelines.MongoDBPipeline&#x27;</span>:<span class="number">500</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中的数值表示优先级，数值越小优先级越高，数据会依从优先级顺序在管道中传递进行处理。</p>
<p>到这里，爬虫开发已经接近完成，下面来考虑一些优化设置。</p>
<p>首先，一些大型网站，或者迭代周期很长的网站，他们的页面中可能存在着一些循环引用的链接，可能是无意造成的，也可能是故意设置，使爬虫陷入死循环，已进行反爬。因此我们因该在爬取时对重复的请求链接进行过滤。scrapy 默认开启了 RFPDupeFilter，通过生成一个 request_seen 文件记录请求指纹，然后在每次发出请求前先检查当前请求是否已经记录过，从而达到过滤重复请求的目的。</p>
<p>然后，当我们爬取的数据量非常大时，request_seen 文件体积将会不断增大，而运行爬虫时需要将该文件读入内存，这可能造成内存占用过大，最终导致程序崩溃。</p>
<p>一种办法是改成使用 Redis 来存储请求指纹，借助 Redis 的高性能，不需要一次性读取全部记录，从而改善默认 RFPDupeFilter 的缺陷，但是数据量太大时，使用 Redis 存储请求指纹仍然会占用大量内存。</p>
<p>这里采用另一种方式，Redis 结合布隆过滤器进行过滤。布隆过滤器的详细原理可以自行上网了解，布隆过滤器的优点是占用空间很少，缺点则是有一定的误判率，使用 Redis 内置的 bitset 可以方便的实现布隆过滤器，下面直接给出代码以及简单解释，在项目目录下新建<code>dupefilter.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> Redis</span><br><span class="line"><span class="keyword">from</span> scrapy.dupefilters <span class="keyword">import</span> BaseDupeFilter</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.request <span class="keyword">import</span> request_fingerprint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .hashmap <span class="keyword">import</span> HashMap</span><br></pre></td></tr></table></figure>
<p>这里引入 scrapy 提供的 <code>BaseDupeFilter</code> 作为自定义 dupefilter 的基类，以及 <code>request_fingerprint</code> 方法生成请求指纹。</p>
<p>引入 python 标准库 <code>hashlib</code> 用于后面使用 md5 和 logging 生成日志。</p>
<p>自定义类<code>HashMap</code>用于生成布隆过滤器使用的哈希函数。</p>
<p>引入 redis 来连接操作 Redis。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,m,seed</span>):</span></span><br><span class="line">        self.m=m</span><br><span class="line">        self.seed=seed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span>(<span class="params">self,value</span>):</span></span><br><span class="line">        ret=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(value)):</span><br><span class="line">            ret+=self.seed*ret+<span class="built_in">ord</span>(value[i])</span><br><span class="line">        <span class="keyword">return</span> (self.m-<span class="number">1</span>)&amp;ret</span><br></pre></td></tr></table></figure>
<p>上面代码中，m 为布隆过滤器需要使用的位大小，seed 用于生成多个 hash 函数，hash 方法则把输入字符串映射到多个位，并且将对应位设置为 1。</p>
<p>下面编写基于 Redis 的布隆过滤器的主体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisBloomDupeFilter</span>(<span class="params">BaseDupeFilter</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, bitSize=<span class="number">32</span>, seeds=[<span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>], blockNum=<span class="number">1</span>, key=<span class="string">&#x27;bloomfilter&#x27;</span></span>):</span></span><br><span class="line">        self.redis = Redis(host=host, port=port, db=db)  <span class="comment"># 连接Redis</span></span><br><span class="line">        self.bitSize = <span class="number">1</span> &lt;&lt; bitSize  <span class="comment"># 在Redis中申请一个BitSet，Redis中BitSet实际上使用String进行存储，因此最大容量为512M，即2^32</span></span><br><span class="line">        self.seeds = seeds  <span class="comment"># 生成多个hash函数的种子</span></span><br><span class="line">        self.key = key  <span class="comment"># Redis中使用的键名</span></span><br><span class="line">        self.blockNum = blockNum  <span class="comment"># Redis中总共申请多少个BitSet</span></span><br><span class="line">        self.hashFunc = []  <span class="comment"># hash函数</span></span><br><span class="line">        <span class="keyword">for</span> seed <span class="keyword">in</span> self.seeds:</span><br><span class="line">            <span class="comment"># 根据提供的种子生成多个hash函数</span></span><br><span class="line">            self.hashFunc.append(HashMap(self.bitSize, seed))</span><br><span class="line">            self.logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span>(<span class="params">cls,settings</span>):</span></span><br><span class="line">        <span class="comment"># scrapy提供的dupefilter中读取爬虫配置的方法</span></span><br><span class="line">        _host = settings.get(<span class="string">&#x27;REDIS_HOST&#x27;</span>, <span class="string">&#x27;localhost&#x27;</span>)</span><br><span class="line">        _port = settings.getint(<span class="string">&#x27;REDIS_PORT&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line">        _db = settings.getint(<span class="string">&#x27;REDIS_DUPE_DB&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">        _bitSize = settings.getint(<span class="string">&#x27;BLOOMFILTER_BIT_SIZE&#x27;</span>, <span class="number">32</span>)</span><br><span class="line">        _seeds = settings.getlist(<span class="string">&#x27;BLOOMFILTER_HASH_SEEDS&#x27;</span>, [])</span><br><span class="line">        _blockNum = settings.getint(<span class="string">&#x27;BLOOMFILTER_BLOCK_NUMBER&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">        _key = settings.get(<span class="string">&#x27;BLOOMFILTER_REDIS_KEY&#x27;</span>, <span class="string">&#x27;bloomfilter&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(_host, _port, _db, _bitSize, _seeds, _blockNum, _key)</span><br></pre></td></tr></table></figure>
<p>在<code>settings.py</code>中加入 Redis 和布隆过滤器相应配置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">REDIS_HOST = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br><span class="line">REDIS_DUPE_DB = <span class="number">0</span></span><br><span class="line">BLOOMFILTER_REDIS_KEY = <span class="string">&quot;bloomfilter&quot;</span></span><br><span class="line">BLOOMFILTER_BLOCK_NUMBER = <span class="number">1</span></span><br><span class="line">BLOOMFILTER_BIT_SIZE = <span class="number">31</span></span><br><span class="line">BLOOMFILTER_HASH_SEEDS = [<span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">31</span>, <span class="number">37</span>]</span><br></pre></td></tr></table></figure>
<p>接着完成<code>dupefilter.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span>(<span class="params">self, request</span>):</span></span><br><span class="line">    fp = request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> self.exists(fp):</span><br><span class="line">        <span class="comment"># 如果请求指纹已经存在</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    self.insert(fp)  <span class="comment"># 如果请求指纹不存在</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert</span>(<span class="params">self, str_input</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加入请求指纹</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    md5 = hashlib.md5()</span><br><span class="line">    md5.update(<span class="built_in">str</span>(str_input).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">    _<span class="built_in">input</span> = md5.hexdigest()</span><br><span class="line">    _name = self.key+<span class="built_in">str</span>(<span class="built_in">int</span>(_<span class="built_in">input</span>[<span class="number">0</span>:<span class="number">2</span>], <span class="number">16</span>) % self.blockNum)</span><br><span class="line">    <span class="keyword">for</span> func <span class="keyword">in</span> self.hashFunc:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将hash映射后的bit为置位为1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        _offset = func.<span class="built_in">hash</span>(_<span class="built_in">input</span>)</span><br><span class="line">        self.redis.setbit(_name, _offset, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span>(<span class="params">self, str_input</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">判断请求指纹是否已存在</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> str_input:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">md5 = hashlib.md5()</span><br><span class="line">md5.update(<span class="built_in">str</span>(str_input).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">_<span class="built_in">input</span> = md5.hexdigest()</span><br><span class="line">_name = self.key+<span class="built_in">str</span>(<span class="built_in">int</span>(_<span class="built_in">input</span>[<span class="number">0</span>:<span class="number">2</span>], <span class="number">16</span>) % self.blockNum)</span><br><span class="line">ret = <span class="literal">True</span></span><br><span class="line"><span class="keyword">for</span> func <span class="keyword">in</span> self.hashFunc:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    如果经过hash映射之后对应的bit位上有任意一个0，则一定不存在</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    _offset = func.<span class="built_in">hash</span>(_<span class="built_in">input</span>)</span><br><span class="line">    ret = ret &amp; self.redis.getbit(_name, _offset)</span><br><span class="line"><span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">self.logger.debug(<span class="string">u&#x27;已过滤的重复请求：%(request)s&#x27;</span>, &#123;<span class="string">&#x27;request&#x27;</span>: request&#125;)</span><br><span class="line">spider.crawler.stats.inc_value(<span class="string">&#x27;redisbloomfilter/filtered&#x27;</span>, spider=spider)</span><br></pre></td></tr></table></figure>
<p>最后，在<code>settings.py</code>中开启自定义 dupefilter：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DUPEFILTER_CLASS = <span class="string">&#x27;zhsc_crawler.dupefilter.RedisBloomDupeFilter&#x27;</span></span><br></pre></td></tr></table></figure>
<p>最后，在做一些反爬优化，首先可以通过为每个请求添加随机 User-Agent 来伪装不同客户端，要实现这一点可以通过scrapy的<code>Downloader Middleware</code>，下载器中间件位于downloader和scrapy engine之间，当engine通知downloader开始从指定url下载数据前，可以在下载器中间件中定义一些预操作。例如，先在<code>settings.py</code>中添加一组 UA，可以从网上找到很多 UA 信息，然后自定义一个<code>ZhscRandomUserAgentMiddleware</code>中间件类，每当downloader开始下载数据前，都对请求头设置一个随机UA：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhscRandomUserAgentMiddleware</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, user_agents=[]</span>):</span></span><br><span class="line">        self.user_agents = user_agents</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls(crawler.settings.getlist(<span class="string">&#x27;UA_Pool&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.user_agents <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(self.user_agents) &gt; <span class="number">0</span>:</span><br><span class="line">            request.headers.setdefault(<span class="string">b&#x27;User-Agent&#x27;</span>, random.choice(self.user_agents))</span><br></pre></td></tr></table></figure>
<p>其中，<code>from_crawler</code>是 scrapy 提供的读取配置文件的类方法，而<code>process_request</code>则是中间件类必须实现的请求处理方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">UA_Pool = [</span><br><span class="line">    <span class="comment"># User-Agent</span></span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/5.0 (Windows NT 10.0;Win64;x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36 Edg/89.0.774.76&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv,2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Windows NT 6.1; rv,2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4094.1 Safari/537.36&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>然后在<code>settings.py</code>中配置使用自定义中间件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="comment">#    &#x27;zhsc_crawler.middlewares.ZhscCrawlerDownloaderMiddleware&#x27;: 543,</span></span><br><span class="line">    <span class="comment"># 发出请求前添加随机UA</span></span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">    <span class="string">&#x27;zhsc_crawler.middlewares.ZhscRandomUserAgentMiddleware&#x27;</span>: <span class="number">800</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里注意要先关闭 scrapy 默认的 UA 中间件。</p>
<p>最后，如果爬虫频繁发出大量请求的话，很容易被做了反爬的网站发现，因此可以通过在<code>settings.py</code>中对爬虫进行限速来预防，scrapy 提供了很方便的实现方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span></span><br><span class="line">AUTOTHROTTLE_ENABLED = <span class="literal">True</span></span><br><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line">AUTOTHROTTLE_START_DELAY = <span class="number">5</span></span><br><span class="line"><span class="comment"># The maximum download delay to be set in case of high latencies</span></span><br><span class="line">AUTOTHROTTLE_MAX_DELAY = <span class="number">60</span></span><br><span class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to</span></span><br><span class="line"><span class="comment"># each remote server</span></span><br><span class="line">AUTOTHROTTLE_TARGET_CONCURRENCY = <span class="number">1.0</span></span><br><span class="line"><span class="comment"># Enable showing throttling stats for every response received:</span></span><br><span class="line">AUTOTHROTTLE_DEBUG = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>到此，整个诗词数据爬虫开发便完成了，完整代码放在我的<a target="_blank" rel="noopener" href="https://github.com/Christopher-Teng/poems_crawler.git">github 仓库</a>。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Scrapy/" rel="tag"># Scrapy</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/05/03/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%AD%EF%BC%89/" rel="prev" title="使用Scrapy爬取诗词数据（续）">
      <i class="fa fa-chevron-left"></i> 使用Scrapy爬取诗词数据（续）
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/21/%E4%BD%BF%E7%94%A8Flask%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E7%94%A8%E6%88%B7%E6%B3%A8%E5%86%8C%E7%99%BB%E5%BD%95%E5%8A%9F%E8%83%BD/" rel="next" title="使用Flask实现简单的用户注册登录功能">
      使用Flask实现简单的用户注册登录功能 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">滕飞</p>
  <div class="site-description" itemprop="description">学海无涯，争渡争渡......</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Christopher-Teng</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  

  

    </div>
</body>
</html>
