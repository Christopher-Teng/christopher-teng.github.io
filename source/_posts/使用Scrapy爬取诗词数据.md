---
title: 使用Scrapy爬取诗词数据
date: 2021-05-02 22:53:56
tags:
  - Python
  - Scrapy
categories:
  - 网络爬虫
---

在本篇中我将使用 Scrapy 来开发一个简单的文本爬虫，从[中华诗词网](https://www.zhsc.net)上爬取诗词数据。

首先对目标网站进行分析，在其主页右下角有一个历代作者的链接入口，从该链接可以进入作者列表页面，如下图中红圈所示：

{% asset_image index.png %}

<!-- more -->

接下来进入作者列表页面，可以看见在页面下方有一个“历代作者”面板区域，其中按历史时期进行划分，收录了各个时代的文人，如下图红圈所示：

{% asset_image authorlist.png %}

打开开发者面板对页面内容进行分析，可以看到该页面包含了所有作者的链接，只不过页面上仅显示当前选中历史时期下的作者列表，而未选中的部分默认使用`style="display:none;"`隐藏了起来，如下图红圈所示：

{% asset_image dom.png %}

仔细浏览以下上面的作者列表页可以发现，中华诗词网收录的文人非常多，而每个人又有很多诗词作品，如果一次全部爬取数据量太大，不仅耗时，而且在不使用代理 IP 的情况下，容易被封，更重要的是，对于传统诗词人们的学习和关注重心都在历史上的著名文人上，而对于全部收录数据来说，这些著名文人的作品只是一小部分。

因此，我将爬虫设计为只对指定的作者进行爬取，而要实现这一点，正好可以从上面的作者列表页作为种子页面启动爬虫。从该页中的所有作者条目中匹配出指定的作者，然后再链接到诗词列表页面，如下图所示：

{% asset_image poemlist.png %}

从诗词列表中，就可以通过每一项的链接到达我们真正要爬取数据的诗词详情页，同时由于某些文人作品很多，采用了分页，因此还需要匹配分页器中的链接地址，依次从所有诗词列表页面中达到详情页爬取数据。

这里需要注意，很多作者都有和别人同名的作品，而且某个作者自己也可能有很多同名的作品，这种情况尤其常见于宋词当中：词牌名相同，正文不同，如上图中红圈所示，柳永名下同为“黄莺儿”的词作便有两首。这里便要考虑爬取数据之后，怎样存取才更方便日后使用的时候查询数据，在这里我使用标识宋词常用的方法：标题=词牌名+正文首句。

最后，来分析我们最终需要爬取数据的诗词详情页，如下图所示，需要爬取的数据有：标题、年代、作者和正文：

{% asset_image poem.png %}

打开开发者工具，仔细分析页面结构，可以看出，需要爬取的数据全部位于`class="shi_neirong"`的 `<div>` 标签下的`class="zh_shi_xiang1"`的 `<div>` 标签内，该 `<div>` 标签包括一个 `<span>` 标签、一个 `<p>` 标签和一个 `<div>` 标签，其中 `<span>` 标签中的是标题，`<p>` 标签中则同时包含了年代和作者，而 `<div>` 标签中的是正文，如下图所示：

{% asset_image poem1dom.png %}

接下来在随机打开多个详情页，确认页面结构是否符合上面的分析，这里为了节省篇幅，只列举出一个和上面详情页稍有不同的页面，如下图所示：

{% asset_image poem2.png %}

{% asset_image poem2dom.png %}

首先从页面上观察可以发现两处不同，一是正文内容中出现了`<p>`，二是正文中多了很多蓝色的文字。

打开开发者工具分析，首先从页面整体结构上看，数据所在的标签位置和层次关系仍然符合上面的分析，如图中横线所示。其次，第一个不同点，多出来的`<p>`应该是网站后台数据处理错误，将 html 的 `<p>` 标签以文本存入了诗词数据中，那么我们之后爬取数据时，应该要对数据进行去 html 标签的处理，第二个不同点，可以看出是对诗文内容的注解，以`<span>`标签的形式插入到正文所在的`<div>`标签中，注解不是我们需要的数据，之后爬取时应该过滤掉，如图中红圈所示。

到这里，对于目标网站的分析基本完成，那么下面开始着手编写爬虫。
